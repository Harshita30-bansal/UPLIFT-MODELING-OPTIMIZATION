{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Create a 1-million–row stratified sample from the full dataset\n",
        "\n",
        "**Why this step?**  \n",
        "The full Criteo uplift dataset has ~13 million rows, which is too large to train models efficiently.  \n",
        "To reduce computation while keeping the dataset representative, we create a **stratified sample**.\n",
        "\n",
        "**What this code does:**\n",
        "\n",
        "1. **Loads** the full 13M dataset.  \n",
        "2. **Decides** the target sample size = 1,000,000 rows.  \n",
        "3. **Computes** the sampling fraction (about 7–8%).  \n",
        "4. **Stratifies** based on both `treatment` and `conversion`  \n",
        "   - ensures the sampled dataset has the same proportions of  \n",
        "     - treated vs control  \n",
        "     - converted vs not converted  \n",
        "5. **Saves** the new 1M-row dataset for all future modeling steps.\n",
        "\n",
        "This results in a smaller dataset that still represents the original distribution.\n"
      ],
      "metadata": {
        "id": "jD_I4SxBGUwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load full dataset (13 million)\n",
        "df = pd.read_csv(\"criteo-uplift-v2.1.csv\")   # replace name\n",
        "\n",
        "print(\"Full dataset size:\", len(df))\n",
        "\n",
        "# 2. Target sample size\n",
        "target_size = 1_000_000\n",
        "\n",
        "# 3. Fraction to sample\n",
        "fraction = target_size / len(df)\n",
        "print(\"Sampling fraction:\", fraction)\n",
        "\n",
        "# 4. Stratified sampling using treatment and conversion\n",
        "df_small = df.groupby(['treatment', 'conversion'], group_keys=False).apply(\n",
        "    lambda x: x.sample(frac=fraction, random_state=42)\n",
        ")\n",
        "\n",
        "print(\"Sampled size:\", len(df_small))\n",
        "\n",
        "# 5. Save the smaller dataset\n",
        "df_small.to_csv(\"sampled_1M.csv\", index=False)\n",
        "print(\"Saved sampled_1M.csv\")\n"
      ],
      "metadata": {
        "id": "hFR5Iw3wGfWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Stratified Train–Test Split (Ensures Balanced Groups)\n",
        "\n",
        "**Goal:**  \n",
        "Split the 1M sampled dataset into Train (80%) and Test (20%) **while keeping the proportions of all 4 groups the same**:\n",
        "- Treated + Visited  \n",
        "- Treated + Not Visited  \n",
        "- Control + Visited  \n",
        "- Control + Not Visited  \n",
        "\n",
        "**How this works:**\n",
        "1. We create a new column `stratify_col` combining `treatment` and `visit`  \n",
        "   → forms 4 balanced groups like `0_0`, `0_1`, `1_0`, `1_1`\n",
        "2. We use `train_test_split(... stratify=stratify_col)`  \n",
        "   → ensures train and test have the same distribution of these groups\n",
        "3. We construct a final `train` dataframe containing:\n",
        "   - feature columns  \n",
        "   - treatment label  \n",
        "   - visit outcome  \n",
        "\n",
        "**Why this is important:**  \n",
        "Random splitting can distort class balance.  \n",
        "Stratified splitting preserves real-world proportions — critical for uplift modeling.\n"
      ],
      "metadata": {
        "id": "aSBTO8NeHYjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Create a stratify column to ensure balance across Treatment AND Outcome\n",
        "# This effectively creates 4 classes: 0_0, 0_1, 1_0, 1_1\n",
        "df_small['stratify_col'] = df_small['treatment'].astype(str) + \"_\" + df_small['visit'].astype(str)\n",
        "\n",
        "# 2. Define X and various y's\n",
        "X = df_small[FEATURE_COLS]\n",
        "y = df_small['visit']\n",
        "t = df_small['treatment']\n",
        "\n",
        "# 3. Split data (80% Train, 20% Test)\n",
        "# Notice we stratify on the NEW column\n",
        "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(\n",
        "    X, y, t,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_small['stratify_col']\n",
        ")\n",
        "\n",
        "# 4. Combine X_train with y and t for EDA purposes ONLY (creating the 'train' variable you missed)\n",
        "train = X_train.copy()\n",
        "train['treatment'] = t_train\n",
        "train['visit'] = y_train\n",
        "\n",
        "print(f\"Train Shape: {train.shape}\")\n",
        "print(f\"Test Shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "9M257ii2HgCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Quick Sanity Check (View Columns & First Few Rows)\n",
        "\n",
        "**Purpose:**  \n",
        "Before moving ahead, we quickly inspect the sampled 1M-row dataset to ensure everything is correct.\n",
        "\n",
        "**What this cell does:**\n",
        "- Prints all column names using `df_small.columns`  \n",
        "- Shows the first 5 rows using `df_small.head()`\n",
        "\n",
        "**Why this is useful:**\n",
        "- Confirms that important columns (`treatment`, `visit`, features like `f0..f11`) exist  \n",
        "- Confirms sampling didn’t remove or damage any columns  \n",
        "- Lets us visually verify that the data looks clean before training models\n"
      ],
      "metadata": {
        "id": "HadsHIxLHu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_small.columns)\n",
        "df_small.head()"
      ],
      "metadata": {
        "id": "75heaEzbHlTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Check Treatment & Outcome Distributions (Sanity Check)\n",
        "\n",
        "**Purpose:**  \n",
        "Verify that the sampled 1M dataset still has the correct proportions of:\n",
        "- Treated vs Control users  \n",
        "- Converted vs Not Converted users  \n",
        "- Visit behavior across treatment groups  \n",
        "\n",
        "**What this cell prints:**\n",
        "\n",
        "1. **Treatment Distribution**  \n",
        "   - Confirms the treatment ratio (Criteo dataset is highly imbalanced ≈ 85% treated).  \n",
        "   - Ensures sampling kept the same ratio.\n",
        "\n",
        "2. **Conversion Distribution**  \n",
        "   - Shows how many users converted vs not.  \n",
        "   - Checks that conversion remains a rare event.\n",
        "\n",
        "3. **Conversion Rate by Treatment**  \n",
        "   - Helps validate uplift signal:  \n",
        "     *Do treated users convert more than control users?*\n",
        "\n",
        "4. **Visit Rate by Treatment**  \n",
        "   - Similar check but for the “visit” outcome.\n",
        "\n",
        "**Why this matters:**  \n",
        "If these distributions look wrong, your sampling corrupted the dataset.  \n",
        "If they look correct, you're safe to proceed to modeling.\n"
      ],
      "metadata": {
        "id": "6XxsuusSH9gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===== Treatment Distribution =====\")\n",
        "print(df_small['treatment'].value_counts(normalize=True))\n",
        "print()\n",
        "\n",
        "print(\"===== Conversion Distribution =====\")\n",
        "print(df_small['conversion'].value_counts(normalize=True))\n",
        "print()\n",
        "\n",
        "print(\"===== Conversion Rate by Treatment =====\")\n",
        "print(df_small.groupby('treatment')['conversion'].mean())\n",
        "print()\n",
        "\n",
        "print(\"===== Visit Rate by Treatment =====\")\n",
        "print(df_small.groupby('treatment')['visit'].mean())\n",
        "print()\n"
      ],
      "metadata": {
        "id": "U0fU63f_H3Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 5: Joint Distribution of Treatment × Conversion\n",
        "\n",
        "**Purpose:**  \n",
        "Check the **combined proportions** of the four key customer groups:\n",
        "- Control + No Conversion  \n",
        "- Control + Conversion  \n",
        "- Treated + No Conversion  \n",
        "- Treated + Conversion  \n",
        "\n",
        "**What this cell computes:**  \n",
        "`df_small.groupby(['treatment', 'conversion']).size() / len(df_small)`\n",
        "\n",
        "This shows what fraction of the sampled dataset lies in each of the 4 categories.\n",
        "\n",
        "**Why this is important:**  \n",
        "Uplift modeling relies on having a healthy mix of treatment and conversion outcomes.  \n",
        "If any of these groups are missing or extremely small, uplift estimation becomes unreliable.\n",
        "\n",
        "This check confirms that **stratified sampling preserved the balance across these groups**.\n"
      ],
      "metadata": {
        "id": "PMTB9sshIQ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small.groupby(['treatment', 'conversion']).size().apply(lambda x: x / len(df_small))\n"
      ],
      "metadata": {
        "id": "Ap_XRj9iIWal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 6: Distribution of All Four Treatment–Conversion Combinations\n",
        "\n",
        "**Purpose:**  \n",
        "Verify how the sampled dataset is distributed across the four combinations of treatment and conversion:\n",
        "\n",
        "- Treated + Converted  \n",
        "- Treated + Not Converted  \n",
        "- Control + Converted  \n",
        "- Control + Not Converted  \n",
        "\n",
        "**What this cell does:**  \n"
      ],
      "metadata": {
        "id": "jCXhDe3EGUc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small.groupby(['treatment', 'conversion']).size() / len(df_small)\n"
      ],
      "metadata": {
        "id": "XZLbrqsCIbTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select outcome, treatment, and feature columns\n",
        "\n",
        "**Purpose:** Define the target outcome and treatment columns and automatically select the feature columns.\n",
        "\n",
        "**Details:**\n",
        "- `OUTCOME_COL` chooses which label we will predict (`'visit'` or `'conversion'`).\n",
        "- `TREATMENT_COL` indicates the column that marks whether a user received the promotion (`'treatment'`).\n",
        "- `FEATURE_COLS` automatically gathers all columns named like `f0, f1, ..., f11`.\n",
        "\n",
        "**Why:** Centralising these choices avoids errors in downstream cells and makes it easy to change the target or treatment variable later.\n",
        "\n",
        "**Note:** The sampled dataset file created earlier is saved as `sampled_1M.csv` (local path: `/mnt/data/sampled_1M.csv`) if you need to reference or reload it.\n"
      ],
      "metadata": {
        "id": "5vFwKHLzJhrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your outcome and treatment\n",
        "OUTCOME_COL = 'visit'        # or 'conversion'\n",
        "TREATMENT_COL = 'treatment'  # later you can try 'exposure'\n",
        "\n",
        "FEATURE_COLS = [c for c in df_small.columns\n",
        "                if c.startswith('f') and c[1:].isdigit()]  # f0..f11\n",
        "\n",
        "# Just to be extra sure:\n",
        "print(FEATURE_COLS, OUTCOME_COL, TREATMENT_COL)\n"
      ],
      "metadata": {
        "id": "R-WYfyZ1JmHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Check Missing Values in Feature Columns\n",
        "\n",
        "**Purpose:**  \n",
        "Before training models, we need to confirm that none of the feature columns (`f0`…`f11`) contain missing values.\n",
        "\n",
        "**What the code does:**\n",
        "- Looks at only the feature columns.\n",
        "- Computes the **fraction of missing values** in each column.\n",
        "- Sorts them from smallest to largest.\n",
        "- Displays the results for quick inspection.\n",
        "\n",
        "**Why this matters:**  \n",
        "Models like Random Forest or Logistic Regression can break or perform poorly if features contain missing values.  \n",
        "This quick check ensures your 1M sampled dataset is clean before proceeding.\n"
      ],
      "metadata": {
        "id": "auvqUKqxJxxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small[FEATURE_COLS].isna().mean().sort_values().head(12)\n"
      ],
      "metadata": {
        "id": "8ztd4YUPJ5Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Check for Infinite Values in Features\n",
        "\n",
        "**Purpose:**  \n",
        "Ensure none of the feature columns contain infinite values (`+inf` or `-inf`), which can break model training.\n",
        "\n",
        "**What the code does:**\n",
        "- Uses `np.isinf()` to detect infinite values in the feature matrix.\n",
        "- `.any()` returns `True` if at least one infinite value exists.\n",
        "\n",
        "**Why important:**  \n",
        "Infinite values can appear from faulty preprocessing or data errors.  \n",
        "Most machine learning models cannot handle them and will fail or give wrong results.\n",
        "\n",
        "**Expected output:**  \n",
        "`False` → your dataset is clean.  \n"
      ],
      "metadata": {
        "id": "V9vW_1kSKFNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.isinf(df_small[FEATURE_COLS]).any()\n"
      ],
      "metadata": {
        "id": "RZYLJGVVJ8tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small[FEATURE_COLS].describe().T\n"
      ],
      "metadata": {
        "id": "yg2AkcjpKCMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 9: Feature Summary Statistics (Describe Table)\n",
        "\n",
        "**Purpose:**  \n",
        "View key descriptive statistics for each feature (`f0`…`f11`) to ensure the data looks normal and consistent before modeling.\n",
        "\n",
        "**What this cell shows for every feature:**\n",
        "- `count` – number of valid entries  \n",
        "- `mean` – average value  \n",
        "- `std` – spread/variability  \n",
        "- `min`, `max` – smallest and largest values  \n",
        "- `25%`, `50%`, `75%` – quartile values  \n",
        "\n",
        "**Why this matters:**  \n",
        "This step helps detect:\n",
        "- unusual value ranges,  \n",
        "- outliers,  \n",
        "- incorrect data types,  \n",
        "- or any signs of corrupted sampling.\n",
        "\n",
        "If all features show stable, reasonable ranges, you can safely continue to modeling.\n"
      ],
      "metadata": {
        "id": "1kW20uhtKbhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Visualize Feature Distributions (Histograms + KDE)\n",
        "\n",
        "**Purpose:**  \n",
        "Plot the distribution of every feature (`f0`…`f11`) in the **training set** to visually inspect whether the values look normal and consistent.\n",
        "\n",
        "**What the code does:**\n",
        "- Creates a large figure with one subplot per feature.  \n",
        "- Uses `sns.histplot()` to show the histogram of each feature.  \n",
        "- Adds a **KDE curve** (smooth density line) to see the overall shape better.\n",
        "\n",
        "**Why this is important:**  \n",
        "Before training uplift models, we need to ensure the feature values:\n",
        "- are not extremely skewed,  \n",
        "- do not contain strange spikes or patterns,  \n",
        "- do not have extreme outliers,  \n",
        "- and behave similarly to the full population.\n",
        "\n",
        "If a feature distribution looks abnormal, it may require transformation or special handling later.\n"
      ],
      "metadata": {
        "id": "FR5Tk-TTKfug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Re-running your histogram logic on the TRAIN set\n",
        "plt.figure(figsize=(18, 24))\n",
        "for i, col in enumerate(FEATURE_COLS):\n",
        "    plt.subplot(4, 3, i+1)\n",
        "    sns.histplot(train[col], bins=50, kde=True, color='teal') # Added KDE for smoothness\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q4lG-e80KY61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#########\n",
        "#f1 --------- Looks almost like a degenerate variable: most customers/users are almost identical w.r.t f1.\n",
        "\n",
        "#That means f1 might carry limited information unless that tiny variation matters for the model.\n",
        "\n",
        "#f2-------strong right skew, many small values, few larger ones.\n",
        "#f3 is left-skewed (bulk near upper bound, thin tail to the left).\n",
        "\n",
        "#features (f0, f2, f3, f5, f6, f9, f11) show long-tailed distributions, either left or right.\n",
        "\n",
        "#scales are different so standard scaler"
      ],
      "metadata": {
        "id": "IYKV3D9KKn9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Boxplot Visualization for Outlier Detection\n",
        "\n",
        "**Purpose:**  \n",
        "Use boxplots to check how each feature (`f0`…`f11`) is distributed and whether any of them contain **extreme outliers**.\n",
        "\n",
        "**What this code does:**\n",
        "- Creates a wide figure to fit boxplots for all features.\n",
        "- Uses `sns.boxplot()` to draw horizontal boxplots for the training features.\n",
        "\n",
        "**Why this is important:**  \n",
        "Boxplots show:\n",
        "- the median of each feature,  \n",
        "- how spread out the values are,  \n",
        "- and whether there are extreme outliers.\n",
        "\n",
        "This helps identify features that may need:\n",
        "- scaling,  \n",
        "- transformation (e.g., log),  \n",
        "- or outlier handling before uplift modeling.\n",
        "\n",
        "**Note:**  \n",
        "A standardized version (after scaling) provides an even clearer view, but this initial raw check is very useful.\n"
      ],
      "metadata": {
        "id": "Ojys-BbxMgox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 10))\n",
        "sns.boxplot(data=X_train[FEATURE_COLS], orient=\"h\", palette=\"Set2\")\n",
        "plt.title(\"Feature Distributions & Outliers (Standardized View recommended later)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gnhjxAk5Ktiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Feature Correlation Heatmap\n",
        "\n",
        "**Purpose:**  \n",
        "Visualize how strongly each feature (`f0`…`f11`) is related to the others in the training dataset.\n",
        "\n",
        "**What this code does:**\n",
        "- Computes the correlation matrix for all feature columns.\n",
        "- Creates a colorful heatmap using `seaborn.heatmap()`.\n",
        "- Dark red/blue areas show strong positive/negative correlations.\n",
        "\n",
        "**Why this is useful:**  \n",
        "- Highly correlated features may carry duplicate information.  \n",
        "- Understanding feature relationships helps during model selection and debugging.  \n",
        "- Even though Random Forest handles correlated features better than linear models,  \n",
        "  it’s still valuable to inspect overall structure.\n",
        "\n",
        "**Tip:**  \n",
        "Set `annot=True` if you want to see exact correlation numbers on the heatmap.\n"
      ],
      "metadata": {
        "id": "4Y_FeSAXMwdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "corr = X_train[FEATURE_COLS].corr()\n",
        "\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    annot=False,      # turn to True if you want numbers\n",
        "    cmap=\"coolwarm\",\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "plt.title(\"Feature Correlation Heatmap (X_train)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vr-pKCFHMmYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Full Preprocessing Pipeline for Uplift Modeling\n",
        "\n",
        "This function performs **all preprocessing steps** needed before uplift modeling.  \n",
        "It takes raw data → cleans it → splits it → transforms it safely → and returns\n",
        "final Train / Validation / Test sets.\n",
        "\n",
        "###  What this function does:\n",
        "\n",
        "### **1. Basic Cleaning**\n",
        "- Normalizes column names  \n",
        "- Drops duplicate rows  \n",
        "- Ensures treatment and outcome are valid 0/1 values  \n",
        "\n",
        "### **2. Feature Selection**\n",
        "- Automatically detects numeric feature columns  \n",
        "- Removes treatment and outcome from the feature list  \n",
        "\n",
        "### **3. Create “strata” column**\n",
        "Creates 4 strata groups:\n",
        "- Treated + Outcome  \n",
        "- Treated + No Outcome  \n",
        "- Control + Outcome  \n",
        "- Control + No Outcome  \n",
        "\n",
        "Used to ensure balanced splits.\n",
        "\n",
        "### **4. Stratified Train/Val/Test Splits**\n",
        "- 80% train/validation  \n",
        "- 20% test  \n",
        "- Train further split into train (80%) and validation (20%)  \n",
        "- All splits use the strata column to preserve treatment/outcome balance  \n",
        "\n",
        "### **5. Preprocessing (trained on train only)**\n",
        "- **Low-variance feature removal**  \n",
        "- **Outlier clipping** using 1st–99th percentile  \n",
        "- **Log transform** on highly skewed features  \n",
        "- **Standard Scaling** (fit on train, applied to all splits)  \n",
        "\n",
        "### **6. Treatment Interaction Features**\n",
        "For each feature `f`, creates `f_x_treat = f * treatment`.\n",
        "\n",
        "### **7. Returns**\n",
        "- `X_train, y_train`  \n",
        "- `X_val, y_val`  \n",
        "- `X_test, y_test`  \n",
        "- Preprocessing metadata:  \n",
        "  - low-variance columns removed  \n",
        "  - log columns  \n",
        "  - clipping thresholds  \n",
        "  - fitted scaler  \n",
        "\n",
        "This pipeline ensures **no data leakage**, consistent scaling, outlier robustness,\n",
        "and uplift-friendly feature engineering.\n"
      ],
      "metadata": {
        "id": "isrLxncgNCnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_uplift(df, treatment_col='treatment', outcome_col='conversion'):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. BASIC CLEANING (SAFE BEFORE SPLIT)\n",
        "    # ---------------------------------------------------------\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    df[treatment_col] = df[treatment_col].astype(int)\n",
        "    df[outcome_col] = df[outcome_col].astype(int)\n",
        "\n",
        "    # Keep only 0/1 rows\n",
        "    df = df[df[treatment_col].isin([0, 1])]\n",
        "    df = df[df[outcome_col].isin([0, 1])]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. IDENTIFY FEATURE COLUMNS\n",
        "    # ---------------------------------------------------------\n",
        "    feature_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    feature_cols = [c for c in feature_cols if c not in [treatment_col, outcome_col]]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. CREATE STRATA FOR UPLIFT-STRATIFIED SPLIT (SAFE BEFORE SPLIT)\n",
        "    # ---------------------------------------------------------\n",
        "    df['strata'] = df[treatment_col].astype(str) + \"_\" + df[outcome_col].astype(str)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. TRAIN/VAL/TEST SPLIT (NO PREPROCESSING DONE YET)\n",
        "    # ---------------------------------------------------------\n",
        "    train_df, test_df = train_test_split(\n",
        "        df, test_size=0.2, random_state=42, stratify=df['strata']\n",
        "    )\n",
        "    train_df, val_df = train_test_split(\n",
        "        train_df, test_size=0.2, random_state=42, stratify=train_df['strata']\n",
        "    )\n",
        "\n",
        "    # Drop helper\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        d.drop(columns=['strata'], inplace=True)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5. PREPROCESSING FIT ONLY ON TRAINING SET\n",
        "    # ---------------------------------------------------------\n",
        "    # 5A. LOW VARIANCE FILTERING (FIT ON TRAIN)\n",
        "    low_var_cols = [c for c in feature_cols if train_df[c].std() < 0.01]\n",
        "\n",
        "    # Remove them from feature list\n",
        "    feature_cols = [c for c in feature_cols if c not in low_var_cols]\n",
        "\n",
        "    # Drop from all splits\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        d.drop(columns=low_var_cols, inplace=True)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5B. OUTLIER CLIPPING — FIT BOUNDS FROM TRAIN ONLY\n",
        "    # ---------------------------------------------------------\n",
        "    clip_bounds = {}\n",
        "    for col in feature_cols:\n",
        "        lower = train_df[col].quantile(0.01)\n",
        "        upper = train_df[col].quantile(0.99)\n",
        "        clip_bounds[col] = (lower, upper)\n",
        "\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        for col in feature_cols:\n",
        "            lower, upper = clip_bounds[col]\n",
        "            d[col] = d[col].clip(lower=lower, upper=upper)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5C. LOG TRANSFORM — SELECT COLUMNS BASED ON TRAIN ONLY\n",
        "    # ---------------------------------------------------------\n",
        "    log_cols = []\n",
        "    for col in feature_cols:\n",
        "        if train_df[col].skew() > 1.0 and train_df[col].max() - train_df[col].min() > 5:\n",
        "            log_cols.append(col)\n",
        "\n",
        "    # Apply log transform\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        for col in log_cols:\n",
        "            d[col] = np.log1p(d[col] - d[col].min() + 1)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5D. STANDARD SCALING — FIT ON TRAIN ONLY\n",
        "    # ---------------------------------------------------------\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_df[feature_cols])\n",
        "\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        d[feature_cols] = scaler.transform(d[feature_cols])\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 6. CREATE TREATMENT INTERACTION FEATURES (SAFE)\n",
        "    # ---------------------------------------------------------\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        for col in feature_cols:\n",
        "            d[f\"{col}_x_treat\"] = d[col] * d[treatment_col]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 7. FINAL TRAIN/VAL/TEST SPLITS\n",
        "    # ---------------------------------------------------------\n",
        "    X_train = train_df.drop(columns=[outcome_col])\n",
        "    y_train = train_df[outcome_col]\n",
        "\n",
        "    X_val = val_df.drop(columns=[outcome_col])\n",
        "    y_val = val_df[outcome_col]\n",
        "\n",
        "    X_test = test_df.drop(columns=[outcome_col])\n",
        "    y_test = test_df[outcome_col]\n",
        "\n",
        "    return (\n",
        "        X_train, y_train,\n",
        "        X_val, y_val,\n",
        "        X_test, y_test,\n",
        "        low_var_cols,\n",
        "        log_cols,\n",
        "        clip_bounds,\n",
        "        scaler\n",
        "    )\n"
      ],
      "metadata": {
        "id": "HZ_A5YNgM7y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Run Full Preprocessing Pipeline on Sampled Dataset\n",
        "\n",
        "**Purpose:**  \n",
        "Execute the `preprocess_uplift()` function to generate the final cleaned and transformed datasets for training, validation, and testing.\n",
        "\n",
        "**What happens here:**  \n",
        "The function:\n",
        "- cleans column names  \n",
        "- removes duplicates  \n",
        "- performs stratified splitting (train/val/test)  \n",
        "- removes low-variance features  \n",
        "- clips outliers  \n",
        "- applies log transforms  \n",
        "- scales numerical features using StandardScaler  \n",
        "- creates treatment–interaction features (`feature * treatment`)  \n",
        "\n",
        "**Outputs returned:**\n",
        "- `X_train`, `y_train` → training set  \n",
        "- `X_val`, `y_val` → validation set  \n",
        "- `X_test`, `y_test` → test set  \n",
        "- `low_var_cols` → removed low-variance columns  \n",
        "- `log_cols` → log-transformed columns  \n",
        "- `clip_bounds` → outlier clipping thresholds  \n",
        "- `scaler` → trained StandardScaler  \n",
        "\n",
        "These are now fully ready to be used for uplift model training.\n"
      ],
      "metadata": {
        "id": "R4nhMaIaNSo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test, low_var_cols, log_cols, clip_bounds,scaler \\\n",
        "    = preprocess_uplift(df_small)\n"
      ],
      "metadata": {
        "id": "uaG3qWmCNOQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Correlation Heatmap of Final Preprocessed Features\n",
        "\n",
        "**Purpose:**  \n",
        "Visualize how the **final cleaned features** relate to each other after full preprocessing  \n",
        "(cleaning → outlier clipping → log transform → scaling → interaction features).\n",
        "\n",
        "**What the code does:**\n",
        "- Computes the correlation matrix for all columns in `X_train`.\n",
        "- Plots a heatmap to show feature relationships using `seaborn.heatmap()`.\n",
        "\n",
        "**Why this step matters:**\n",
        "- Preprocessing can significantly change feature behavior.\n",
        "- Helps verify that:\n",
        "  - No two features are excessively correlated (redundant).\n",
        "  - Interaction features (`*_x_treat`) behave as expected.\n",
        "  - There are no strange artifacts introduced during preprocessing.\n",
        "\n",
        "**Tip:**  \n",
        "Dark red = strong positive correlation.  \n",
        "Dark blue = strong negative correlation.  \n",
        "Lighter colors = weak correlation (usually preferred).\n"
      ],
      "metadata": {
        "id": "oohHlVY4NdHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "corr = X_train.corr()\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap (All Final Features)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ikOCElNNNY5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16: Correlation Heatmap — Original Features Only\n",
        "\n",
        "**Purpose:**  \n",
        "Examine the correlations among only the **original feature columns** (`f0…f11`), without the interaction features (`*_x_treat`).\n",
        "\n",
        "**What this code does:**\n",
        "- Filters out all interaction features.\n",
        "- Computes correlation on the original features only.\n",
        "- Plots a clean heatmap to show relationships among the raw signals.\n",
        "\n",
        "**Why this is important:**\n",
        "- Interaction features can dominate the heatmap and hide important patterns.\n",
        "- This view helps identify:\n",
        "  - highly correlated original features,\n",
        "  - potential redundancies,\n",
        "  - any distortions introduced during preprocessing.\n",
        "\n",
        "Using this, you can better understand the dataset structure before building uplift models.\n"
      ],
      "metadata": {
        "id": "SGsl-OYlNqTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_cols = [c for c in X_train.columns if not c.endswith(\"_x_treat\")]\n",
        "sns.heatmap(X_train[orig_cols].corr(), cmap=\"coolwarm\")\n",
        "plt.title(\"Heatmap — Original Features Only\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "66rOXQBwNipC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 17: Compare Feature Correlations — Treated vs Control\n",
        "\n",
        "**Purpose:**  \n",
        "Understand how feature relationships differ between:\n",
        "- users who received treatment (promotion)\n",
        "- users who did not receive treatment\n",
        "\n",
        "This is directly connected to uplift modeling because changes in correlation patterns can signal treatment effects.\n",
        "\n",
        "### What this code does:\n",
        "\n",
        "1. **Splits** the training data into treated (`train_treat`) and control (`train_ctrl`) groups.\n",
        "2. **Removes** the `treatment` column from analysis (to avoid trivial correlations).\n",
        "3. **Computes** separate correlation matrices for treated and control users.\n",
        "4. **Calculates** the *difference* between these matrices:\n"
      ],
      "metadata": {
        "id": "wbu47yjGN2y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate groups\n",
        "train_treat = X_train[X_train['treatment'] == 1]\n",
        "train_ctrl = X_train[X_train['treatment'] == 0]\n",
        "\n",
        "# Drop treatment itself to avoid trivial correlations\n",
        "cols = [c for c in X_train.columns if c != 'treatment']\n",
        "\n",
        "corr_treat = train_treat[cols].corr()\n",
        "corr_ctrl = train_ctrl[cols].corr()\n",
        "\n",
        "# Difference\n",
        "corr_diff = corr_treat - corr_ctrl\n"
      ],
      "metadata": {
        "id": "JWI1HODhNyCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 18: Visualize Correlation Differences (Treatment vs Control)\n",
        "\n",
        "**Purpose:**  \n",
        "Plot the difference between feature correlations in the treated and control groups to understand how treatment changes user behavior.\n",
        "\n",
        "**What the heatmap shows:**\n",
        "- **Red areas** → Feature pairs that become *more correlated* under treatment  \n",
        "- **Blue areas** → Feature pairs that are *more correlated* in the control group  \n",
        "- **White areas** → No significant difference\n",
        "\n",
        "**Why this is useful:**  \n",
        "Changes in correlation patterns often reveal how treatment influences relationships between features.  \n",
        "This helps identify which feature interactions might contribute to uplift and where causal effects appear.\n",
        "\n",
        "The heatmap provides a clear visual summary of these structural differences.\n"
      ],
      "metadata": {
        "id": "0XaY5jWZAMUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "sns.heatmap(corr_diff, cmap=\"coolwarm\", center=0, linewidths=0.5)\n",
        "plt.title(\"Correlation Difference: Treatment vs Control\\n(Positive = stronger in treatment)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eqxeRjO7N7eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 19: Feature Importance Analysis — Treated vs Control (Uplift Importance)\n",
        "\n",
        "**Purpose:**  \n",
        "Identify which features contribute differently to predictions in the treated and control groups.  \n",
        "This helps reveal which features drive **uplift** (treatment effect).\n",
        "\n",
        "### What the code does:\n",
        "\n",
        "1. **Splits** the training data into:\n",
        "   - treated users (`treatment = 1`)\n",
        "   - control users (`treatment = 0`)\n",
        "\n",
        "2. **Trains two independent Random Forest models:**\n",
        "   - `model_treat` → predicts outcomes for treated users  \n",
        "   - `model_ctrl`  → predicts outcomes for control users  \n",
        "\n",
        "3. **Computes feature importances** for each model.\n",
        "\n",
        "4. **Calculates uplift importance:**  \n"
      ],
      "metadata": {
        "id": "TPzIgswyBRoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split into separate treatment/control feature sets\n",
        "X_treat = X_train[X_train['treatment'] == 1].drop(columns=['treatment'])\n",
        "y_treat = y_train[X_train['treatment'] == 1]\n",
        "\n",
        "X_ctrl  = X_train[X_train['treatment'] == 0].drop(columns=['treatment'])\n",
        "y_ctrl  = y_train[X_train['treatment'] == 0]\n",
        "\n",
        "# Fit two independent models\n",
        "model_treat = RandomForestClassifier(\n",
        "    n_estimators=300, random_state=42\n",
        ")\n",
        "model_ctrl = RandomForestClassifier(\n",
        "    n_estimators=300, random_state=42\n",
        ")\n",
        "\n",
        "model_treat.fit(X_treat, y_treat)\n",
        "model_ctrl.fit(X_ctrl, y_ctrl)\n",
        "\n",
        "# Get importances\n",
        "imp_treat = model_treat.feature_importances_\n",
        "imp_ctrl  = model_ctrl.feature_importances_\n",
        "\n",
        "# Compute uplift importance\n",
        "uplift_importance = imp_treat - imp_ctrl\n",
        "\n",
        "# Pack into DataFrame\n",
        "import pandas as pd\n",
        "df_imp = pd.DataFrame({\n",
        "    \"feature\": X_treat.columns,\n",
        "    \"importance_treat\": imp_treat,\n",
        "    \"importance_ctrl\": imp_ctrl,\n",
        "    \"uplift_importance\": uplift_importance\n",
        "}).sort_values(\"uplift_importance\", ascending=False)\n",
        "\n",
        "df_imp.head(10)\n"
      ],
      "metadata": {
        "id": "a7a3I15oBcp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 20: Visualize Uplift Feature Importance\n",
        "\n",
        "**Purpose:**  \n",
        "Plot the uplift importance values (`importance_treat - importance_ctrl`) to see which features behave differently for treated vs control users.\n",
        "\n",
        "**What the barplot shows:**\n",
        "- **Positive bars (red)** → feature is more important in the *treated* model  \n",
        "  → indicates potential to identify users who benefit from treatment  \n",
        "- **Negative bars (blue)** → feature is more important in the *control* model  \n",
        "  → may indicate users who don’t need treatment  \n",
        "- **Near-zero bars** → feature behaves similarly in both groups\n",
        "\n",
        "**Why this matters:**  \n",
        "Uplift modeling is all about finding what separates treatment responders from non-responders.  \n",
        "This visualization reveals which features hold the strongest uplift signal.\n"
      ],
      "metadata": {
        "id": "PVW-mb7YBr9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 12))\n",
        "sns.barplot(\n",
        "    data=df_imp.sort_values(\"uplift_importance\", ascending=False),\n",
        "    x=\"uplift_importance\", y=\"feature\", palette=\"coolwarm\"\n",
        ")\n",
        "plt.title(\"Uplift Feature Importance (Treat - Control)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wEXNPY_lBhC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 21: Compute Validation Uplift (Two-Model Method)\n",
        "\n",
        "**Purpose:**  \n",
        "Estimate the individual uplift for each validation user using the classical two-model approach:\n",
        "- `model_treat` predicts the probability of outcome **if treated**\n",
        "- `model_ctrl` predicts the probability of outcome **if not treated**\n",
        "\n",
        "### What this code does:\n",
        "\n",
        "1. **Prepare validation features**\n",
        "   - Keep a copy with the treatment column  \n",
        "   - Remove `treatment` from the features passed into the models (models should not use it)\n",
        "\n",
        "2. **Predict `p1`**  \n",
        "   Probability the user would visit if **given treatment**:\n"
      ],
      "metadata": {
        "id": "VWLNVR7FB3ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Predict `p0`**  \n",
        "Probability the user would visit if **not treated**:\n"
      ],
      "metadata": {
        "id": "JOfsDEq4B8by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Compute uplift**\n"
      ],
      "metadata": {
        "id": "jN3oV-nmB_b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Positive → user benefits from promotion  \n",
        "- Near zero → promotion doesn't change behavior  \n",
        "- Negative → user may react poorly (rare)\n",
        "\n",
        "**Why this is important:**  \n",
        "This gives the validation‐set uplift estimates used to evaluate model performance  \n",
        "and eventually select customers with uplift > 0 (Easy Problem).\n"
      ],
      "metadata": {
        "id": "OTGes5z0CCyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_treat = X_val.copy()  # full features including treatment column\n",
        "X_val = X_val.drop(columns=['treatment'])\n",
        "\n",
        "# predict P(Y=1 | T=1, X)\n",
        "p1 = model_treat.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# predict P(Y=1 | T=0, X)\n",
        "p0 = model_ctrl.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# uplift = difference\n",
        "uplift_val = p1 - p0\n"
      ],
      "metadata": {
        "id": "URBiFKgfBxE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 22: Qini Curve & Uplift Evaluation Functions\n",
        "\n",
        "These functions evaluate how good our uplift model is at ranking customers.\n",
        "\n",
        "###  `compute_qini_curve()`\n",
        "Builds the Qini uplift curve by:\n",
        "- sorting users by predicted uplift\n",
        "- calculating cumulative treated and control outcomes\n",
        "- computing incremental gain as we target more users\n",
        "\n",
        "This curve shows how much better our uplift model performs compared to random targeting.\n",
        "\n",
        "---\n",
        "\n",
        "###  `compute_auuc()`\n",
        "Computes **AUUC (Area Under the Uplift Curve)**.\n",
        "A higher AUUC means better uplift performance.\n",
        "\n",
        "---\n",
        "\n",
        "###  `compute_qini_coefficient()`\n",
        "Computes the **Qini coefficient**, which adjusts for the random baseline.\n",
        "Interpretation:\n",
        "- Positive → model is better than random  \n",
        "- Zero → model = random  \n",
        "- Negative → model performs worse than random  \n",
        "\n",
        "---\n",
        "\n",
        "###  `plot_qini_curve()`\n",
        "Draws the Qini curve along with the random baseline for visualization.\n",
        "\n",
        "---\n",
        "\n",
        "**Why these functions matter:**  \n",
        "They allow us to objectively judge whether our uplift model truly identifies customers who gain more from treatment — crucial for validating any uplift approach.\n"
      ],
      "metadata": {
        "id": "evqK3F2fCb80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_qini_curve(y_true, treatment, uplift_scores):\n",
        "    \"\"\"\n",
        "    Compute Qini uplift curve.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        \"y\": y_true,\n",
        "        \"t\": treatment,\n",
        "        \"uplift\": uplift_scores\n",
        "    }).sort_values(\"uplift\", ascending=False)\n",
        "\n",
        "    df[\"y_treat\"] = df[\"y\"] * df[\"t\"]\n",
        "    df[\"y_ctrl\"]  = df[\"y\"] * (1 - df[\"t\"])\n",
        "\n",
        "    df[\"cum_treat\"]   = df[\"t\"].cumsum()\n",
        "    df[\"cum_control\"] = (1 - df[\"t\"]).cumsum()\n",
        "\n",
        "    df[\"cum_y_treat\"]   = df[\"y_treat\"].cumsum()\n",
        "    df[\"cum_y_control\"] = df[\"y_ctrl\"].cumsum()\n",
        "\n",
        "    # avoid division by zero\n",
        "    df[\"qini\"] = df[\"cum_y_treat\"] - (\n",
        "        df[\"cum_y_control\"] * df[\"cum_treat\"] / df[\"cum_control\"].replace(0, np.nan)\n",
        "    )\n",
        "\n",
        "    return df[\"qini\"].fillna(0).values\n",
        "\n",
        "\n",
        "def compute_auuc(qini_curve):\n",
        "    \"\"\"\n",
        "    Area under uplift/Qini curve.\n",
        "    \"\"\"\n",
        "    return np.trapz(qini_curve) / len(qini_curve)\n",
        "\n",
        "\n",
        "def compute_qini_coefficient(qini_curve):\n",
        "    \"\"\"\n",
        "    Qini coefficient = AUUC - area under random targeting curve.\n",
        "    \"\"\"\n",
        "    n = len(qini_curve)\n",
        "    random_baseline = np.linspace(0, qini_curve[-1], n)\n",
        "    area = np.trapz(qini_curve - random_baseline) / n\n",
        "    return area\n",
        "\n",
        "\n",
        "def plot_qini_curve(qini_curve):\n",
        "    \"\"\"\n",
        "    Plot Qini / uplift curve.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(qini_curve, label=\"Qini Curve\")\n",
        "\n",
        "    # random baseline\n",
        "    plt.plot(np.linspace(0, qini_curve[-1], len(qini_curve)),\n",
        "             linestyle=\"--\", label=\"Random baseline\")\n",
        "\n",
        "    plt.title(\"Uplift (Qini) Curve\")\n",
        "    plt.xlabel(\"Number of samples (sorted by uplift)\")\n",
        "    plt.ylabel(\"Incremental Gain\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "icRgjHhTCNJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 23: Evaluate Uplift Model using Qini / AUUC\n",
        "\n",
        "**Purpose:**  \n",
        "Evaluate how well the model ranks users by their expected uplift using Qini and AUUC metrics.\n",
        "\n",
        "**What this block does:**\n",
        "1. Builds the Qini curve by sorting users by predicted uplift and computing cumulative incremental gain.\n",
        "2. Computes AUUC (area under the uplift curve): higher means better ranking quality.\n",
        "3. Computes the Qini coefficient (AUUC adjusted for random baseline): positive values indicate performance better than random.\n",
        "4. Plots the Qini curve together with the random baseline for visual inspection.\n",
        "\n",
        "**How to interpret results:**\n",
        "- `AUUC` (printout): larger → better model ranking.  \n",
        "- `Qini` (printout): should be **positive**; larger values are better.  \n",
        "- **Plot**: curve above dashed baseline (especially at left) means the model successfully finds users who benefit most from treatment.\n",
        "\n",
        "**Notes:** Use these metrics on validation/test sets (never on training) to avoid optimistic estimates.\n"
      ],
      "metadata": {
        "id": "7KXOf4Q0CtHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qini_curve = compute_qini_curve(\n",
        "    y_true = y_val.values,\n",
        "    treatment = X_val_treat['treatment'].values,\n",
        "    uplift_scores = uplift_val\n",
        ")\n",
        "\n",
        "print(\"AUUC:\", compute_auuc(qini_curve))\n",
        "print(\"Qini:\", compute_qini_coefficient(qini_curve))\n",
        "\n",
        "plot_qini_curve(qini_curve)\n"
      ],
      "metadata": {
        "id": "dGfjO8hFCQuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}